<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0"/><link rel="preload" as="image" href="/me.jpg"/><link rel="preload" as="image" href="https://twitter.com/favicon.ico"/><link rel="preload" as="image" href="https://github.com/favicon.ico"/><link rel="preload" as="image" href="https://scholar.google.com/favicon.ico"/><title>Joe Edelman</title><link rel="stylesheet" href="/page.css"/><center id="welcome"><div class="me"><h2>Joe Edelman</h2><div class="facebox"><img class="face" src="/me.jpg"/><div class="col"><a target="_" class="icon" href="http://twitter.com/edelwax"> <img src="https://twitter.com/favicon.ico"/> </a><a target="_" class="icon" href="http://github.com/jxe"> <img src="https://github.com/favicon.ico"/> </a><a target="_" class="icon" href="https://scholar.google.com/citations?hl=en&amp;user=ZCQjxd4AAAAJ"> <img src="https://scholar.google.com/favicon.ico"/> </a></div></div></div></center><div class="nav" id="current"><b>Current</b><a href="#philosophy">Philosophy</a><a href="#writing">Writing</a><a href="#prototypes">Prototypes</a><a href="#games">Games</a><a href="#background">Background</a></div><article><p>I co-lead the <a href="https://meaningaligment.org">Meaning Alignment Institute</a>, where I work to align markets, democracies, and AIs with what&#39;s important to people. There, my focus is on new, values-explicit <a href="https://meaningalignment.substack.com/p/the-first-moral-graph">democratic</a> and post-market structures. I also curate <a href="https://universe.meaningalignment.org">a database of what&#39;s good about life</a>, and field-build towards a paradigm shift in social choice, AI alignment, and mechanism design.</p>
<p>I also helped start <a href="http://turtlemafia.org">the Turtle Mafia</a>, a support group for researchers.
    </p>
</article><div class="nav" id="philosophy"><a href="#current">Current</a><b>Philosophy</b><a href="#writing">Writing</a><a href="#prototypes">Prototypes</a><a href="#games">Games</a><a href="#background">Background</a></div><article><p>My philosophy work descends pretty clearly from that of <a href="https://en.wikipedia.org/wiki/Charles_Taylor_(philosopher)">Charles Taylor</a>, <a href="http://ruthchang.net">Ruth Chang</a>, <a href="https://en.wikipedia.org/wiki/Amartya_Sen">Amartya Sen</a>, and <a href="http://nyu.academia.edu/DavidVelleman">David Velleman</a>.</p>
<p>It concerns the nature of values and norms, and how they play into the choices we make, and into our retrospective assessments. That is, I work mainly in the theories of choice, action, and practical reason.</p>
<p>My biggest contribution is definitions for <a href="https://arxiv.org/abs/2404.10636">&quot;human values&quot;</a> and <a href="https://github.com/jxe/vpm/blob/master/vpm.pdf">“meaningful choices&quot;</a> that are precise enough to create surveys, metrics, aligned ML models, new democratic mechanisms, etc. Perhaps it will also lead to explainable, moral learning in AI, and offer a path past mechanisms that optimize for engagement and revealed preference, rather than underlying values.</p>
<p>My deepest motivation is not just to contribute to philosophy, but to answer pressing questions like:</p>
<ul>
<li>Why are some human needs sensed/addressed by markets and bureaucracies, but not others?</li>
<li>Is there a metric it&#39;s safe to maximize?</li>
<li>What drives the modern trend towards atomization and social isolation?</li>
</ul>
<p>I believe these are ultimately questions about what in human life is worth honoring, and that the answers are found in the details of how people make choices, and how they assess them. E.g.: What do people mean when they say an experience was <em>meaningful</em> (as opposed to pleasurable, important, etc) or a choice was <em>wise</em> (as opposed to effective, clever, etc)?</p>
<p><em>Read more at MAI&#39;s <a href="https://meaninglabs.notion.site/Related-Academic-Work-c933408fd8fc44c3acd42d6ccb827461?pvs=4">Related Academic Work</a> page.</em>
    </p>
</article><div class="nav" id="writing"><a href="#current">Current</a><a href="#philosophy">Philosophy</a><b>Writing</b><a href="#prototypes">Prototypes</a><a href="#games">Games</a><a href="#background">Background</a></div><article><p><a class="essay" href="https://arxiv.org/abs/2404.10636"><b>What are Human Values, and How Do We Align AI to them?<!-- --> (<!-- -->2024<!-- -->)</b>; with Oliver Klingefjord and Ryan Lowe<!-- --> <i>There is an emerging consensus that we need to align AI systems with human values (Gabriel, 2020; Ji et al., 2024), but it remains unclear how to apply this to language models in practice. We split the problem of &quot;aligning to human values&quot; into three parts: first, eliciting values from people; second, reconciling those values into an alignment target for training ML models; and third, actually training the model. In this paper, we focus on the first two parts, and ask the question: what are &quot;good&quot; ways to synthesize diverse human inputs about values into a target for aligning language models?</i> </a></p><p><a class="essay" href="https://www.youtube.com/watch?v=hZpKdfbrd6o"><b>Scale, Togetherness, and Meaning<!-- --> (<!-- -->2023<!-- -->)</b> <i>In Chapter 1 of this talk, I cover problems with meaning and the social fabric that emerge at large scales—such as with global social networks, operating systems, app stores, and markets, and give solutions.</i> <!-- -->(<a href="https://textbook.sfsd.io/overview">transcript</a>)</a></p><p><a class="essay" href="https://github.com/jxe/vpm/blob/master/vpm.pdf"><b>Values, Preferences, Meaningful Choice<!-- --> (<!-- -->2022<!-- -->)</b> <i>I present a conception of values as attention policies resulting from constitutive judgements, and use it to build an alternative preference relation, Meaningful Choice, which retains many desirable features of revealed preference.</i> </a></p><p><a class="essay" href="https://textbook.sfsd.io"><b>Values-Based Data-Science and Design<!-- --> (<!-- -->2021<!-- -->)</b> <i>A textbook on the design of meaningful social apps, focusing especially on methods for gathering sources of meaning, understanding why it&#x27;s hard for people to live meaningfully, designing apps that help with these &quot;hard steps&quot;, and monitoring the results.</i> </a></p><p><a class="essay" href="https://medium.com/what-to-build/nothing-to-be-done-bfe2ce71a3a2"><b>Nothing to be Done<!-- --> (<!-- -->2017<!-- -->)</b> <i>Intellectual history of the west, from a designers&#x27; standpoint, as a succession of approaches to human systems.</i> </a></p><p><a class="essay" href="https://medium.com/what-to-build/is-anything-worth-maximizing-d11e648eb56f"><b>Is Anything Worth Maximizing<!-- --> (<!-- -->2016<!-- -->)</b> <i>About how metrics affect the structure of organizations and societies, how they change the economy, how we’re doing them wrong, and how we could do them right.</i> </a></p><p><a class="essay" href="http://nxhx.org/Choicemaking/"><b>Choicemaking and the Interface<!-- --> (<!-- -->2014<!-- -->)</b> <i>Theories of choice from economics and philosophy suggest information requirements for good choices. In view of these requirements, we can see why current menus lead toward regrettable and isolating choices.</i> </a></p></article><div class="nav" id="prototypes"><a href="#current">Current</a><a href="#philosophy">Philosophy</a><a href="#writing">Writing</a><b>Prototypes</b><a href="#games">Games</a><a href="#background">Background</a></div><article><p><a class="essay" href="https://chat.openai.com/g/g-TItg5klMA-values-discovery"><b>Values Discovery GPT<!-- --> (<!-- -->2023<!-- -->)</b> <i>A GPT which talks to you about what&#x27;s important to you and makes values cards which capture your sources of meaning.</i> </a></p><p><a class="essay" href="https://meaningalignment.substack.com/p/the-first-moral-graph"><b>Moral Graph Elicitation<!-- --> (<!-- -->2023<!-- -->)</b> <i>A democratic process for collecting values from a large, diverse group and building a &quot;moral graph&quot; that shows consensus about which values are wiser than which, and which can be used to create policy, tune LLMs, etc.</i> </a></p><p><a class="essay" href="https://twitter.com/edelwax/status/1367881670389547010"><b>Building a Second Heart<!-- --> (<!-- -->2021<!-- -->)</b> <i>A research demo for a writing interface backed by a database of values.</i> </a></p><p><a class="essay" href="/pdf/edelman-habitat.pdf"><b>Social Programming Considered as a Habitat for Groups<!-- --> (<!-- -->2019<!-- -->)</b> <i>A new way to code up social apps and information systems emerges from studying how people use ordinary speech to set up social roles and obligations.</i> </a></p></article><div class="nav" id="games"><a href="#current">Current</a><a href="#philosophy">Philosophy</a><a href="#writing">Writing</a><a href="#prototypes">Prototypes</a><b>Games</b><a href="#background">Background</a></div><article><p>Find my games on <a href="https://meaning.supplies/supplies/list/qtyu4XHY">meaning.supplies</a></p></article><div class="nav" id="background"><a href="#current">Current</a><a href="#philosophy">Philosophy</a><a href="#writing">Writing</a><a href="#prototypes">Prototypes</a><a href="#games">Games</a><b>Background</b></div><article><p>My origins are in HCI and in game design.</p>
<p>In tech, I was lucky to learn from people like <a href="https://en.wikipedia.org/wiki/Alan_Kay">Alan Kay</a>, <a href="https://hci.stanford.edu/winograd/">Terry Winograd</a>, and <a href="https://en.wikipedia.org/wiki/Bill_Verplank">Bill Verplank</a> at <a href="https://en.wikipedia.org/wiki/Interval_Research_Corporation">Interval Research</a>, from <a href="http://www.caseyfenton.com/">Casey Fenton</a> at CouchSurfing (where I developed the <a href="/csmetrics/">metrics</a> which guided the company), from <a href="https://people.csail.mit.edu/hes/">Howie Shrobe</a> and <a href="https://web.media.mit.edu/~minsky/">Marvin Minksy</a> at MIT. And more recently through conversations with <a href="http://worrydream.com">Bret Victor</a> and <a href="http://rmozone.com/">Rob Ochshorn</a>.</p>
<p>My tactic of running social experiments through games and performance emerged from study with <a href="https://en.wikipedia.org/wiki/Christian_Wolff_(composer)">Christian Wolff</a> (partipatory music) and <a href="https://en.wikipedia.org/wiki/Peter_Parnell">Peter Parnell</a> (playwriting) at Dartmouth, and then various improvisational scores with <a href="http://nancystarksmith.com">Nancy Stark Smith</a>, <a href="http://www.mikevargas.net">Mike Vargas</a>, <a href="http://www.actiontheater.com/ruth.htm">Ruth Zaporah</a>, and others. I had the great fortune to work alongside <a href="http://www.lethalbeef.com/">Albert Kong</a> and <a href="http://paradeofkites.com/">Catherine Herdlick</a> on the real world games festival <a href="http://www.comeoutandplay.org/">Come Out and Play</a>.</p>
<p>My concern with meaning and metrics started when I developed the meaning-based organizational metrics at Couchsurfing.com, then co-founded the Center for Humane Technology with Tristan Harris, and coined the term “Time Well Spent” for a family of metrics adopted by teams at Facebook, Google, and Apple.</p>
<p>I then started <a href="https://sfsd.io">an online school</a> and wrote <a href="https://textbook.sfsd.io">a textbook on Values-Based Design</a>, and finally launched <a href="https://meaningalignment.org">a nonprofit</a> to bring about a future where wise AIs and humans collaborate to help people live well.</p>
<p>I&#39;ve benefited from working alongside <a href="https://twitter.com/ellie__hain">Ellie Hain</a>, Oliver Klingefjord, <a href="https://scholar.google.ca/citations?user=iRgYMuEAAAAJ&hl=en/">Ryan Lowe</a>, and (earlier) <a href="http://www.tristanharris.com/">Tristan Harris</a>, Nathan Vanderpool, and Anne Selke.
    </p>
</article><div style="height:160px"></div>